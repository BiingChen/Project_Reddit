{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Initial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of Notebook\n",
    "- Convert raw JSON data into dataframe\n",
    "- Remove duplicate posts\n",
    "- Convert target variable from string to integer\n",
    "- Light feature engineering\n",
    "- Train Test Split data\n",
    "- Export X and y data for use later in workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts JSON into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_to_df(posts, features = ['subreddit', 'author', 'title', 'selftext', 'created_utc', 'num_comments']):\n",
    "    feat_dict = [{feat : post['data'][feat] for feat in features}  for post in posts]\n",
    "    return pd.DataFrame(feat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in raw JSON data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/raw.json', 'r') as f:\n",
    "    raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['subreddit', 'author', 'title', 'selftext', 'created_utc', 'num_comments','score','over_18',\n",
    "                'score']\n",
    "df = posts_to_df(raw,features=feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove numbers from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].str.replace('\\d+', '')\n",
    "df['title'] = df['title'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-text data and lemmatize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function does the following:\n",
    "1. Uses Beautiful Soup to remove HTML Markup\n",
    "2. Uses regex to remove any remaining non-text data\n",
    "3. Converts all text to lower case\n",
    "4. Lemmatizes each word converting it into its base/dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatize_text = True\n",
    "\n",
    "def clean_text(raw_text): \n",
    "    bs_text = BeautifulSoup(raw_text, 'lxml').get_text()\n",
    "    only_text = re.sub(\"[^a-zA-Z]\", \" \", bs_text)\n",
    "    words = only_text.lower().split()\n",
    "    if lemmatize_text == True:\n",
    "        word_list = [lemmatizer.lemmatize(word) for word in words]\n",
    "    else:\n",
    "        word_list = [word for word in words]\n",
    "    return \" \".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'/'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=bJujIwtdkw&amp;t=s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.bloomberg.com/view/articles/--/russia-attacked-u-s-troops-in-syria\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=tjounoU\n",
      "\n",
      ":\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=-Obeu_VYY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.youtube.com/watch?v=uQVTPuE_qRU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.liveleak.com/view?i=_\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/yibingchen/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://www.instagram.com/p/BjVnrKFaI/?taken-by=eminem\n",
      "\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "df.title = df.title.map(clean_text)\n",
    "df.selftext = df.selftext.map(clean_text);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light feature engineering \n",
    "- Calculate number of characters for title and text\n",
    "- Create boolean field:  title_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_len'] = df.title.str.len()\n",
    "df['text_len'] = df.selftext.str.len()\n",
    "df['title_only'] = (df.text_len == 0)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert target variable (subreddit) to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map = {'confessions': 0, 'Jokes':1}\n",
    "df['subreddit_int'] = df['subreddit'].map(subreddit_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Cleansed DataFrame for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/df_clean.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup target variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df['subreddit_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup feature variables X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author and Create Date are also dropped since they are not good predictors of sub-reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels=['subreddit','subreddit_int','author','created_utc'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export y_train and y_test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open('../Data/y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export X_train and X_test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open('../Data/X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
