{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Initial Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of Notebook\n",
    "- Convert raw JSON data into dataframe\n",
    "- Remove duplicate posts\n",
    "- Convert target variable from string to integer\n",
    "- Light feature engineering (Get length of title and self text)\n",
    "- Train Test Split data\n",
    "- Export X and y data for use later in workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from bs4 import BeautifulSoup      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_to_df(posts, features = ['subreddit', 'author', 'title', 'selftext', 'created_utc', 'num_comments']):\n",
    "    feat_dict = [{feat : post['data'][feat] for feat in features}  for post in posts]\n",
    "    return pd.DataFrame(feat_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in raw data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/raw.json', 'r') as f:\n",
    "    raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['subreddit', 'author', 'title', 'selftext', 'created_utc', 'num_comments','score','over_18',\n",
    "                'score']\n",
    "df = posts_to_df(raw,features=feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicates from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove numbers from text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].str.replace('\\d+', '')\n",
    "df['title'] = df['title'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(raw_text):  #Bing's copy\n",
    "    bs_text = BeautifulSoup(raw_text, 'lxml').get_text()\n",
    "    only_text = re.sub(\"[^a-zA-Z]\", \" \", bs_text)\n",
    "    words = only_text.lower().split()\n",
    "    #stem_words = [p_stemmer.stem(word) for word in words]\n",
    "    stem_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(stem_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title = df.title.map(clean_text)\n",
    "df.selftext = df.selftext.map(clean_text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my brother death took a major toll on my father and it took him year to go back to my brother room i believe part of him ha decided to let go of the grief and is trying to renovate my late brother old bedroom i wa helping him move out thing and stumbled upon drawing and poem my brother used to write i looked at the date and it said sept which happens to be about month before the tragic event no one in my family know i found his journal i don t want to bring it up in the case i find his suicide note i took every bit of paper i found and am thinking of going through it today i am unsure if i have the power to look through them i don t know if i am strong enough my mind ha lingered with thought on why he would do that for year i have mixed emotion i want to know what wa going through his head but i do not want the heartbreak of knowing that i couldn t help him sooner almost six year later and i still think of him every single day i miss you so much brother update i want to thank everyone for giving me tremendous support through here i greatly appreciate everyone i went through his journal and to my discovery it wa mostly drawing they are very interesting i did find a few journal entry but it wasn t anything like i expected he did not leave a note instead i found some research of topic that interested him his note make his drawing make sense it only left me with more question than answer overall i am very glad and thankful to have found them i feel like he left them for me because i will truly cherish them forever thank you for the support reddit'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selftext[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert target variable (subreddit) to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_map = {'confessions': 0, 'Jokes':1}\n",
    "df['subreddit_int'] = df['subreddit'].map(subreddit_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup target variable y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(df['subreddit_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup feature variables X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels=['subreddit','subreddit_int'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light feature engineering \n",
    "- Since author and create time is not very useful for predicting which subreddit\n",
    "- Get number of characters for title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['title_len'] = X.title.str.len()\n",
    "X['text_len'] = X.selftext.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export y_train and y_test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "    \n",
    "with open('../Data/y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export X_train and X_test objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "    \n",
    "with open('../Data/X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
